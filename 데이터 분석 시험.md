# SNS 중독

data.csv는 사람들의 연령대와 성별, 자주 사용하는 SNS를 정리한 데이터입니다.

data.csv 파일의 각 변수는 다음과 같습니다.

- age : 연령대 (ex. 10대, 20대, 30대 등)
- gender : 성별 (남성은 M, 여성은 F)
- sns : 자주 사용하는 sns
- time : 일평균 sns 이용 시간

data.csv 파일의 값을 이용하여 아래 지시사항을 수행하세요.

## 지시사항

1. sns 접속 시간으로 내림차순 정렬하세요.
2. sns 접속 시간 상위 20명을 추출하여 새로운 데이터를 생성하세요.
3. 그룹을 만드는 함수를 활용하여 새로운 데이터의 연령과 sns 값에 따라 그룹을 만들고, 각 그룹별 평균 sns 접속 시간을 산출하여 출력하세요.

```
'''
data.csv
age     gender  sns         time
60      M       Twitter     7.05
60      F       Facebook    11.85
...
'''

import pandas as pd
import numpy as np

df = pd.read_csv("data.csv")
# print(df)

sort_df = df.sort_values("time", ascending=False)
# print(sort_df)
sorted_df = sort_df[:20]
# print(sorted_df)


group_df = sorted_df.groupby(['age','sns']).mean()
print(group_df['time'])
```

# 다이아몬드의 컷과 투명도

연마되지 않은 다이아몬드의 원석은 그렇게 아름답지 않습니다. 다이아몬드의 반짝임을 결정하는 중요한 기술이 바로 컷인데요, 최근에 들어서야 다이아몬드의 컷 기술에 따라 다이아몬드의 광채가 크게 달라진다는 것이 밝혀져 등급 기준에 포함되었습니다.

고액 다이아몬드들을 컷과 투명도로 분류하여 평균값들을 알아보도록 하겠습니다.

데이터 프레임의 각 칼럼은 다음과 같습니다.
```
carat	cut	color	clarity	depth
캐럿	컷	색깔	투명도	깊이
table	price	x	y	z
상단 면적	가격	x축 크기	y축 크기	z축 크기
```

## 지시사항

1. diamonds.csv 파일에 다이아몬드 데이터가 저장되어 있습니다. 이를 불러와 데이터 프레임으로 저장하세요.

2. 데이터 프레임에서 가격(price)이 10000 이상인 다이아몬드만 추출하세요.

3. 추출한 데이터 프레임을 “컷(cut)과 투명도(clarity)” 순서로 그룹을 지으세요.(그룹을 지을 때 반드시 순서에 유의하셔야 합니다.)

4. 지어진 그룹에 평균 함수를 적용하여 가격이 낮은 순서에서 높은 순서로 출력하세요.

```
import pandas as pd
import numpy as np
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
'''
출력 형식을 위한 스켈레톤 코드입니다.
아래 줄 부터 문제에 맞는 코드를 작성해주세요.
'''

import pandas as pd

df = pd.read_csv("diamonds.csv")

exp_df = df[df["price"]>=10000]

group_df = exp_df.groupby(['cut', 'clarity']).mean()

sort_df = group_df.sort_values(by=('price'),ascending=True)
print(sort_df)
```

# 트럼프 대통령 트윗 분류하기

주어진 트럼프 대통령의 트윗 메세지를 받아 해시태그(#), 멘션(@), 메세지로 분류하는 함수trump_tweet(text)를 작성하세요.

## 지시사항

trump_tweet 함수가 text를 공백을 기준으로 nn개의 문자열로 나누고, 각각의 나뉘어진 문자열을 아래의 규칙을 따라 분류하도록 구현하세요.

- 각 문자열이 '#'로 시작하면 'Hashtag'로 분류하여 리스트에 저장합니다.
- 각 문자열이 '@'로 시작하면 'Mention'로 분류하여 리스트에 저장합니다.
- 이외의 경우는 묶어서 따로 분류하여 리스트에 저장합니다.
- @와 #문자는 단어를 분류하는 과정에서 포함되지 않습니다.

각각 분류된 리스트를 아래 지정된 형식처럼 출력되도록 print 함수를 이용하세요.

```
def trump_tweet(text) :
    # 주어진 규칙에 맞추어 trump_twit()함수를 구현해주세요.
    # pass는 지우고 코드를 작성해주세요.
    Hashtag = []
    Mention = []
    lst = []
    split = text.split()
    
    for word in split :
        if word.startswith('#') :
            word = word.replace('#', '')
            Hashtag.append(word)
        elif word.startswith('@') :
            word = word.replace('@', '')
            Mention.append(word)
        else :
            lst.append(word)
            
    print("hash list : ", Hashtag,"\n","mention list : ", Mention,"\n", "text list : ", lst)

t = input()
print(trump_tweet(t))
```

# 영어 단어 빈도수 찾기

corpus.txt 파일은 특정 문서를 분석한 결과, 발견되는 모든 영어단어와 그 빈도를 저장한 문서입니다. (오른쪽 실습 창에서 corpus.txt 파일을 직접 열어보실 수 있습니다.)

corpus.txt 파일에는 영어 단어와 해당 단어의 빈도수가 각각 /(슬래쉬)를 기준으로 나뉘어져 쓰여져있습니다.

이 파일에서, 주어진 text로 시작하는 모든 단어와 그 빈도를 출력하는 기능을 하는 함수 filter_by_text(text)를 구현하세요.

## 지시사항

1. corpus.txt에 있는 모든 단어와 빈도수를 tuple의 형태로 리스트corpus에 추가하세요.
2. 리스트 corpus 에 저장된 데이터 중에서 text변수의 문자열로 시작하는 단어만을 추려 리스트 result에 저장하세요.
3. 리스트 result 에 저장된 데이터를 빈도수를 기준으로 내림차순 정렬하여 20개까지 출력합니다. 데이터가 20개 미만일 경우 모두 출력하세요.

```
def filter_by_text(text) :
    # 주어진 규칙에 맞추어 filter_by_text()함수를 구현해주세요.
    # corpus.txt에 있는 텍스트를 읽어와서 corpus라는 리스트에 추가한다.
    corpus = []
    with open('corpus.txt') as file :
        for line in file :
            word, freq, *rest = line.split("/")
            freq = int(freq.replace('\n', ''))
            corpus.append((word,freq))
    # print(corpus)
    # corpus에 있는 데이터 중, text로 시작하는 단어만을 추려서 result라는 리스트에 저장한다.
    result = [(word,freq) for word, freq in corpus if word.startswith(text)] 
    # print(result)
    
    # 찾은 영어 단어를 빈도수를 기준으로 내림차순으로 정렬하여 20개만 출력한다.
    print(sorted(result, key = lambda result : result[1], reverse = True)[:20])
    
t = input()
filter_by_text(t)

'''
'10555\ninterestingly', '693\ninteresting',
'''
```

# 넷플릭스 시청 데이터 분석하기

netflix.json 파일에는 유저별 시청한 영화 정보가 담겨져 있습니다. 데이터의 key는 영화 코드이고 value는 해당 영화를 감상한 유저 코드 리스트가 주어집니다.

movies.py에 titles 딕셔너리는 key가 영화 코드를 정수 자료형으로, value는 해당 영화의 제목을 문자열 자료형으로 담고 있습니다.

titles = {1: 'Dinosaur Planet', ... }

## 지시사항

get_top_movies 함수에 대한 설명입니다.

1. 매개변수 n : 양의 정수 n이 주어집니다.
2. 반환값 : netflix.json 데이터에 주어진 영화 중, 시청한 유저의 수가 n회 이상의 영화들만 저장된 리스트를 유저의 수에 대한 내림차순으로 반환하세요.
3. 반환값 형식 : 반환되는 리스트의 각 요소는 두 개의 자료 s와 n을 가진 튜플 (s, n)이어야 합니다. s는 문자열 자료형이며 영화 제목을, n은 정수 자료형이며 영화를 시청한 유저 수를 나타냅니다.

예를 들어 n이 23500이라면, 아래처럼 시청 유저 수가 23500 이상인 영화들만 리스트에 담겨 내림차순으로 반환되어야 합니다.

```
[('Pirates of the Caribbean: The Curse of the Black Pearl', 24786), ('Forrest Gump', 24557), ('The Sixth Sense', 24284), ('The Matrix', 23956), ("Ocean's Eleven", 23891), ('Independence Day', 23879), ('Spider-Man', 23649)]
```

```
import json
from movies import titles
import pandas as pd

def get_top_movies(n) : # 매개변수 n : 양의 정수 n이 주어집니다.
    # netflix.json 데이터를 이용하여 주어진 문제를 해결하세요.
    # 파일에는 유저별 시청한 영화 정보가 담겨져 있습니다. 데이터의 key는 영화 코드이고 value는 해당 영화를 감상한 유저 코드 리스트가 주어집니다.
    
    with open('netflix.json') as f:
        for key, value, *rest in f :
            if len(value) >= n :
                print(key)
            
    
    # 반환값 : netflix.json 데이터에 주어진 영화 중, 시청한 유저의 수가 n회 이상의 영화들만 저장된 리스트를 유저의 수에 대한 내림차순으로 반환하세요.
    # 반환값 형식 : 반환되는 리스트의 각 요소는 두 개의 자료 s와 n을 가진 튜플 (s, n)이어야 합니다. s는 문자열 자료형이며 영화 제목을, n은 정수 자료형이며 영화를 시청한 유저 수를 나타냅니다.

def main():
    n = int(input())
    print(get_top_movies(n))

if __name__ == "__main__":
    main()
```